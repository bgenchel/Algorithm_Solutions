CSE 101: Algorithms Review

********** Overview **************

Running Time
	- T(n): the number of ‘computer steps’ taken 
	- log(n) < sqrt(n) < n < nlog(n) < n^2 < 2^n


Fibonacci Numbers

	Lemma: Asymptotic Growth Rate
 
		Fn >= 2^(n/2) for n >= 6

		Proof: by Induction
			Base: F6 = 8 = 2^(6/2), F7 = 13 > 2^(7/2)
			Thus, For n > 7
				Fn = F_n-1 + F_n-2 >= 2^((n-1)/2) + 2^((n-2)/2)
							        =  2^((n-2)/2)*(sqrt(2) + 1)	   
							  		> 2^((n-2)/2)*2 = 2^(n/2)

		DP Implementation
			function Fib(n):
				if n <= 1:
					return n
				create an array F[0 … n]
				F[0] = 0, F[1] = 1
				for i from 2 to n:
					F[i] = F[i-1] + F[i-2]
				return F[n]

			- Running time is linear (each value of F from 1 to n is evaluated only once)
			- Linear number of arithmetic operations but quadratic number of bit operations



Sorting and Searching
	- covers count sort, but i'll leave that to the actual sorting and searching section.
	
	Searching for two elements with a given sum
		
		Naive Method:
			function SumSearch(A[1...n], S):
				for i from 1 to n:
					for j from i+1 to n:
						if A[i] + A[j] = S:
							return i, j
				return -1 //if not found

			- Running time: O(n^2), Theta(n^2)

		Two Pointer Method:
			function SumSearchTwoPointers(A[1...n], S):
				//A is sorted
				l = 1, r = n
				while l < r:
					if A[l] + A[r] = S:
						return l, r
					else if A[l] + A[r] > S:
						r--
					else if A[l] + A[r] < S:
						l++
				return -1 //if not found

			- Running time: O(n)



********** Divide and Conquer **************

Integer Multiplication 
	- Integer Addition: O(n), just add the values at each bit, carries aren't too expensive.
	- Integer Multiplication: O(n^2), whole of one number needs to be multiplied by each digit of the other.

	- Recurrent Formula:
		- y = 2*floor(y/2), y even
		    = 1 + 2*floor(y/2), y odd

		=> x*y = 2*(x*floor(y/2)), y even
			  = x + 2*(x*floor(y/2))

		Psuedocode: 
			function Multiply(x,y): //input is two n-bit integers
				if y = 0: 
					return 0
				z = multiply(x, floor(y/2))
				if y is even:
					return 2*z
				else:
					return x + 2*z

		- running time: O(n^2), number of bits in y decreases by 1 at each recursive call

	- Better Recurrent Formula:
		- x = 2^(n/2)*xL + xR //left bits are shifted into place
		  y = 2^(n/2)*yL + yR

		=> x*y = (2^(n/2)*xL + xR)*(2^(n/2)*yL + yR)
			   = 2^(n)*xL*yL + 2^(n/2)*(xL*yR + xR*yL) + xRyR

		- This still includes 4 multiplications => 4 recusive calls; can be improved

			- compute 3 recursive calls by substituting:
				(xL*yR + xR*yL) = (xL + xR)*(yL + yR) - xL*yL - xR*yR

			- Psuedocode: 
				function Karatsuba(x,y): //input is two integers in binary
					n = max(size of x, size of y)
					if n = 1:
						return x*y
					xL = left ceil(n/2) bits of x //shift right to get
					xR = right floor(n/2) bits of x
					yL = left ceil(n/2) bits of y
					yR = right floor(n/2) bits of y

					P1 = Karatsuba(xL, yL)
					P2 = Karatsuba(xR, yR)
					P3 = Karatsuba(xL + xR, yL + yR)

					return 2^(2*floor(n/2)) + (P3 - P1 - P2)*2^(floor(n/2)) + P2

				- recurrent relation: T(n) = 3*T(n/2) + O(n)
					=> running time = Sum[i=0:k] 3^i*c*n/2*i
									= c*n*Sum[i=0:k] (3/2)^i
									= c*n*Sum[i=0:log2(n)] //k is the number of levels in the recursion tree = log(n)
									= c*n*O(3^(log2(n))/2^(log2(n)))
									= O(n^log2(3))
									= O(n^1.584)



Recurrence Relations
		- Master theorem:
			for recursive problem with T(n) = a*T(ceil(n/b)) + O(n^d)
				T(n) = O(n^d), d > logb(a)
					 = O(n^d*log(n)), d = logb(a)
					 = O(n^logb(a)), d < logb(a)

			proof: A problem with the above recurrence relation will recursively split down into levels that have a^i problems of size (n/(b^i) 
			such that the sum of all the work done is 
				Sum[i=0:k] a^i*c*(n/(b^i))^d = c*n^d*Sum[i=0:k] (a/(b^d))^i
										     = c*n^d*Sum[i=0:logb(n)] (a/(b^d))^i

				
					- if b^d = a (i.e. d = logb(a))
						T(n) = c*n^d*Sum[i=0:logb(n)] 1
							 = c*n^d*logb(n) 
							 = O(n^d*log(n))

					- if b^d > a (i.e. d > logb(a))
						T(n) = c*n^d*Sum[i=0:logb(n)] (some value less than 1)^i
							 = c*n^d*1
							 = O(n^d)

					if b^d < a (i.e. d < logb(a))
						T(n) = c*n^d*Theta(a^(logb(n))/b^(d*logb(n)))
							 = c*n^d*Theta(n^logb(a)/n^d)
							 = O(n^logb(a))


Matrix Multiplication
	- Naive algorithm:
		for X*Y = Z, Z[i,j] = Sum[k=1:n] X[i,k]*Y[k,j]
		
		PsuedoCode:
			function NaiveMatMult(X, Y):
				Z = new matrix with X.numrows rows and Y.numcolumns columns
				initialize all Z elements to 0
				for i in 1 to X.numrows:
					for j in 1 to Y.numcolumns:
						for k in X.numcolumns (= Y.numrows):
							Z[i,j] += X[i,k]*Y[k,j]


				return Z

			- time complexity is O(n^3) (general for nxn matrices)

	- Block multiplication generalization:
		- X = [X11 X12]  Y = [Y11 Y12]
			  [X21 X22]      [Y21 Y22]

		 => X*Y = [X11*Y11 + X12*Y22  X11*Y21 + X12*Y22]
		 		  [X21*Y11 + X22*Y21  X21*Y21 + X22*Y22]

		- 8 distinct multiplications implies
			- T(n) = 8*T(n/2) + O(n^2) //recombination of nxn matrices is n^2 additions

			- by master theorem this is still O(n^3)

		- Improvement: Strassen algorithm 
			- essence is that you can write the multiplication in terms of 7 more abstract terms that each contain 1 multiplication
			- This reduces recurrence to T(n) = 7*T(n/2) + O(n^2) => O(n^(log2(7))) = O(n^2.807) slightly less than for our original formulation




********** Data Structures **************

Dynamic Array
	- Changes size dynamically to fit elements being added to it.

	- Additive Reallocation Scheme
		- increase size of array by certain number (k) each time capacity is reached and then shift all entries over.
		- total time for all reallocations = k + 2*k + 3*k + ... n 
									 	   = k*(1 + 2 + ... n/10) 
									 	   = k*(n/10)*(n/10 - 1)/2 
									       = k*n^2/200 - k*n/20
									       = O(n^2)

		- because size is increased so often (about n), average (amortized) time for inserting one new element is O(n)

	- Multiplicative Reallocation Scheme
		- double size of array whenever capacity is reached
		- total time for all reallocations = 1 + 2 + 4 + .. + n 
										   = 2*n
										   = O(n)

		- because size is increased by so much, average (amortized) running time for inserting element is O(1).



Heap
	- Priority Queue
		- Implements these methods: 
			- Insert(p): adds new element with priority p
			- Remove(it): removes element pointed to by iterator it
			- GetMin(): returns an element with minimal priority
			- ExtractMin(): extracts an element with minimal priority
			- ChangePriority(it, p): changes priority of element pointed to by it to p

		- Naive Implementations
			- Array
				- just trys to peform these methods by iteratively searching through an unsorted array where the elements are stored
				- GetMin() takes O(n) because one has to compare every element

			- Sorted Array
				- GetMin() is O(1) because the minimum element will always be at the front
				- Remove takes O(n) because elements have to be sifted towards the front

			- Sorted List
				- GetMin() is O(1), just take the head of the list
				- Remove is O(1), only have to change the head pointer
				- Insert is O(n), have to go through the list to find the right spot.

		- Best practice is to implement with a heap, which has O(log(n)) for all operations.

	- Binary (Min) Heap
		- satisfies (min) heap property: every node is <= to each of its children.

		- Priority Queue Implementation
			- GetMin():
				- min value is at the root => O(1)

			- Insertion: 
				- Insert and Sift Up: Put the new element as a leaf, then bubble it up the tree 
				  (trade places with lesser parents) until its in the right spot.

				- Running time = O(h) where h is the height of the tree.
							   = O(log(n))

				- invarient: at any point in time, the heap property is violated at at most one vertex

			- ExtractMin:
				- ExtractMin and SiftDown: Take the root out, replace it with a leaf. Then, move the new 
				  root down the tree, swapping it with its smaller child, until its in the 'right' spot.

				- Again, Running time = O(h) = O(log(n))

			- ChangePriority:
				- change the priority and let the changed element sift up or down 
				depending on whether its priority decreased or increased.

				- b/c sifting operation, run time is O(log(n))

			- Remove:
				- change the priority of the element to -inf, let it sift up then extract the minimum.

				- two sifting operations => O(2*log(n)) = O(log(n))

	- Heap in Array
		- for a complete binary tree, levels are filled in from left to right; all rows full except last one

		- fill array level by level from left to right

		- vertex number i has parent floor(i/2) and children 2*i and 2*i+1

	- 2 choice problems to review in Stepic section



Segment Trees
	- Solves (Dynamic) range minimum/sum query problem:
		- given: array A[1 ... n] of integers
		- operations:
			- Query(l, r): returns the minimum value/sum of the elements of the subarray A[l ... r]

			- Update(i, x) changes the value of A[i] to x 

		- Naive Solution (linear search)
			- Query(l, r) = O(r - l + 1)
			- Update(i, x) = O(1), since everything is just stored in place in an array


	- Segment Tree:
		- tree in which root represents whole array and child nodes represent half the range of 
		  their parents down to individual leaves. 
		- For the min/sum query problem, each node holds the min value or sum for its designated range.

		- Construction takes O(n) time:
			- Start from leaves; one node for each individual leaf; then, 'combine' leaves by creating 
			  parent nodes that contain the min value/sum between the two. Set the leaves as children to this node. 
			  Repeat until all leaves have been joined. 

			- Time complexity is n + n/2 + ... + 1 = 2n => O(n)

		- Update():
			- When a leaf value is updated, all parent values up to the root must adapt the change. That's h levels to check, or O(log(n))

		- Query():
			- segments represented by tree branches are called canonical, they represent perfect division of the range of the array.	

			- any conceivable segment of the original set can be represented as a disjoint union of O(log(n)) canonical segments that can be found recursively.

			- PsuedoCode:
				function QuerySegmentTree(l, r, node_l, node_r, node):
					if [node_l:node_r] = [l:r] or fits inside [l:r]:
						return node.min_value //the value contained at the node

					if [l:r] is non intersecting with [node_l:node_r] (completely outside its range): 
						return inf

					halfway = floor((node_l + node_r)/2)
					return min(QuerySegmentTree(l, r, node_l, halfway, node.left), QuerySegmentTree(l, r, halfway, node_r, node.right))

				- searching through a tree, so running time is O(log(n))



Disjoint Sets
	- Overarching disjoint sets datastructure implements the following methods: 
		- MakeSet(x): creates a singleton set consisting of x
		- Union(x,y): merges the two sets containing x and y respectively.
		- Find(x): returns an ID of the set containing x

	- Psuedocode:
		function MakeSet(x):
			insert x into forest of sets 
			set its parent/ID to itself
			set its rank to 0

		function Find(x):
			pull the node x from array/hashtable storage, follow parent pointers up until the current node is its own parent, then return that node/ID

		function Union(x,y):
			rx = Find(x)
			ry = Find(y)
			if rx = ry: 
				return

			if rank(rx) > rank(ry):
				parent(ry) = rx
			else: 
				parent(rx) = ry
				if rank(rx) = rank(ry):
					rank(ry)++

	- Lemma: Any node of rank k has at least 2^k nodes in its tree
		- Corollary: running time of Find and Union is O(log(n))

	- Path Compression:
		- when find is called, parents can be called recursively, allowing a reference to the root to then be sent back through 
		  the recursion so that each node on the path can set its parent directly to the root, reducing find and union times.



********** Sorting **********

Sorting Basics
	- Measure efficiency by number of comparisons.
	- Stable Sort: sort in which elements deemed the same by value remain ordered as they were in the original set.

	- Stable Counting Sort O(n + M):
		Psuedocode:
			function CountSort(A[1...n]): //input is array of n integers
				create an array B[1 ... M], initialize with 0's
				for i from 1 to n:
					B[A[i]] = B[A[i]] + 1
				for j from 2 to M:
					B[j] = B[j] + B[j-1]
				for j from n to 1:
					A'[B[A[j]]] = A[j]
					B[A[j]] = B[A[j]] - 1

		High Level Description:
			Create an Array B that contains M elements where M is the largest integer in the original set.
				Initialize all elements to 0; this array will keep track of the number of each element in each set.
			Iterate through the initial array, 
				for each element, increment the element's count in B.  
			Then iterate through B, adding up the counts of previous elements such that B[i] = B[i] + B[i - 1].
				after this loop, B[i] is the number of elements in A that are <= i
			such that at the end, the Mth element contains the total number of elements in the original array.
			Finally, sort as follows:
				for each element in the original array,
					place it in the position in the new aray corresponding to its current count in the M-element array.
					after placing it, decrement that count. 

					during this loop, B[i] is the rightmost vacant position for an element i.

		Time Complexity: 
			- Iterate through n-element original array twice, Iterate through the M-element count array twice
			  => O(n + M)

	- Radix Sort
		- For a list of numbers the largest of which has d digits,
			perform d counting sorts, sorting the list by digit value starting with the least significant digit. 

			because count sort is stable, starting with the least significant digit guarantees the numbers will sort correctly for higher digits.

		- time complexity of d count sorts = O(d*n), not d*(n+M), since the size of the numbers no longer effects the method. 

	- Insertion Sort
		- High Level Description
			Starting with the second element, iterate through the list.
				for each element, iterate backwards through its previous elements, 
					swap the current element with its previous if its previous is greater.

				because we perform this process at each element, we can be sure that all elements prior 
				to the current are in order; each step in the forward iteration is just a matter of putting 
				the new element in the correct spot in a growing, sorted array. 

		- Psuedo Code
			function InsertionSort(A[1...n]):
				for i from 2 to n:
					j = i
					while j > 1 and A[j] < A[j-1]:
						swap A[j] and A[j-1]
						j = j-1

		- Time Complexity
			T(n) = 1 + 2 + 3 + .... + n = O(n^2)



	- Selection Sort
		- High Level Description
			iterate through the array
				for each element, search from the current element to the end of the array and find the minimum
				element. Then swap it with the current element.

		- Psuedo Code
			function SelectionSort(A[1...n]):
				for i from 1 to n:
					k = i
					for j from i+1 to n:
						if A[j] < A[k]:
							k = j //(A[k] is the min of A[i ... n]
					swap A[i] and A[k]

		- Time Complexity
			T(n) = n + n - 1 + ... + 1 = O(n^2)

	- Lower Bound for Comparison Sorts 
		- Comparison sorts can be modeled as decision trees where the number of leaves is at least n! for all possible permutations (compare each element to every other) 

		=> worst case number of comparisons is log2(n!) = Omega(nlogn)
			this is because n! > (n/2)^(n/2) 


Heap Sort
	- Description 
		Given an array A[1 ... n] insert all its elements into a min-heap. Then extract the min n times.

		Sorts in place (no extra space needed)
		not stable  

	- PsuedoCode //Heap sort in place
		function HeapSort(A[1...n]):
			BuildMaxHeap(A)
			size = n
			for i from n to 2:
				swap A[size] and A[1]
				size--
				SiftDown(A, 1)

	- Time Complexity
		Build Heap function is T(n) = 2*n = O(n)
		Extract the min n times = O(nlog(n)) for n logn extract and siftdown operations.

		=> total time for algorithm is O(nlog(n))



Merge Sort
	- Description
		Recursively split the array in half until at singletons;
		Recombine ("Merge") Sets togeher in sorted order on the way back up.

	- PsuedoCode
		function MergeSort(A[1...n], l, r):
			if l >= r:
				return
			middle = floor((l+r)/2)
			return Merge(MergeSort(A, l, m), MergeSort(A, m+1, r)) 

		function Merge(A[1...n], B[1...m]): //inputs are sorted
			Create new array C with n+m elements to be the output.
			i = 1, j = 1 //indexes into A and B
			for k from 1 to n+m:
				if A[i] <= B[j]:
					C[k] = A[i]
					i++
				else
					C[k] = B[j]
					j++
			return C

	- Time Complexity:
		Merge is O(n)
		=> T(n) <= 2*T(n/2) + O(n)
		   by the master theorem, complexity is O(nlog(n))

	- takes O(n) extra space for holder arrays for merge.



Quick Sort
	- Sorts in place, no extra space needed

	- Psuedocode:
		function QuickSort(A[1...n], l, r):
			if l >= r:
				return 

			m = Partition(A, l, r)
			QuickSort(A, l, m - 1)
			QuickSort(A, m + 1, r)

		function Partition(A[1...n], l, r)
			x = A[l]
			j = l
			for i from l+1 to r: 
				if A[i] <= x:
					j++
					swap A[j] and A[i]
			swap A[l] and A[j]
			return j

	- Pivots can be good or bad. If the pivots are bad, 
	  Algorithm can be O(n^2). Bad pivots partition the set unevenly; they are uneven.
	  	- balance reduces the number of comparisons needed

	- Pivots depend on the initial ordering of the set, when picking pivots from the left.

	- Observation: Half of the elements of A guarante a blanaced partition.
	=> By selecting random pivots, we can acheive an average running time of O(nlog(n))
		do this by picking a random element and swapping it with A[l] before partitioning.

	- Intro Sort
		runs quick sort with a simple deterministic pivot selection heuristic (say, median of the first, middle, and last element).

		if the recursion depth exceeds a certain threshold, clog(n) the algorithm switches to heap sort.



********** Decomposition of Graphs **********

Graphs
	- Ways to Represent
		- Edge List: Exactly what it sounds like.
			- space: Theta(|E|)
			- find if edge exists: Theta(|E|)
			- find all neighbors of u: Theta(|E|)

		- Adjacency Matrix: nxn matrix where element ij being 1 indicates an edge between nodes i and j.
			- space: Theta(|V|^2)
			- find if edge exists: Theta(1)
			- neighbors of u: Theta(|V|)

		- Adjacency List: Each node stores a list of nodes it is connected to.
			- space: Theta(|V| + |E|)
			- find if edge exists: deg(u)
			- neighbors of u: deg(u)



Depth-First Search in Undirected Graphs
	- Finding all nodes reachable from a particular node:
		- Psuedocode:
			function Explore(v):
				visited[v] = true
				previsit(v)
				for each edge (v, u) in E:
					if visited[u] = false:
						Explore(u)
				postvisit(v)

	- Depth-first Search:
		- Psuedocode
			- from the slides:
				function DFS(G):
					for all nodes v:
						visited[v] = false
					for all nodes v:
						Explore(v)

			- my interpretation:
				function DFS(v):
					if visited[v] = true:
						return

					visited[v] = true
					previsit(v)
					for each of v's neighbors:
						DFS(neighbor)
					postvisit(v) 


		- Running time
			- O(|V| + |E|) since Explore is called exactly once for each
				node v and each edge is examined either once (directed) or 
				twice (undirected)

	- Connected Components (of Undirected Graphs)
		- def: an inclusion-wise maximal subset of vertices such that there is a path between any two of them.

		- finding connected components:
			- Psuedocode
				function ConnectedComponents(G):
					initialize all nodes to not visited

					num_cc = 0
					iterate through all nodes in v:
						if visited[v] = false:
							num_cc++	
							DFS(v)



Depth-First Search in Directed Graphs
	- Lemma: a directed graph has a cycle iff its dfs search reveals a back edge.

	- find if directed graph contains a cycle:
		function CheckForCycles(G):
			initialize two arrays pre and post visited to keep track for each node in G.

			iterate through the nodes:
				for each node: perform dfs s.t.
					previsit
						recursively visit
					postvisit

			if at anypoint a node is visited that is previsited but not post visited, this implies that a cycle exists.	

	- Topological Ordering
		- def: linearization of a directed graph; linear ordering of its vertices suc hthat for any edge(u, v), u comes before v.

		- lemma: a directed graph can be linearized iff it is a DAG
			- aka if there is a cycle the graph cannot be linearized

		- each DAG contains at least one source and at least one sink.
			=> to linearize, find a source, output it, delete it from the graph.

		- Psuedocode:
			For Graph G with V vertices
				Create array visited size V initialized to false
				Create array indegrees size V initialized to 0 //keeps track of edges pointing towards nodes.
				Create a stack called sources to hold source nodes.
				for each vertex in G:
					for each neighbor of vertex:
						indegrees[neighbor]++;

				for each vertex in G:
					if indegrees[vertex] is 0:
						push vertex onto sources

				while sources is not empty:
					pop a source off the stack
					Remove source from G
					for each of source's neighbors:
						indegrees[neighbor]--;

						if indegrees[neighbor] is 0:
							push it onto the stack, its now a source

				









				





				
